# These are used when processing the data into chunks by the prep_docs script
# The CORPUS_RAW_FILE_DIR should be wherever you checked out The Corpus of Cthulhu on your machine
# The repo is: git@github.com:jensawyer/corpus_of_cthulhu.git
# If you don't want to use that, you should provide a directory with some txt files we can work with.
CORPUS_RAW_FILE_DIR=/Users/jennifersawyer/dev/datasets/corpus_of_cthulhu/txts
# The jsonl file is created by the prep_docs script in this repo. It's made of chunks and encodings that
# this project expects.
CORPUS_JSONL_FILE=/Users/jennifersawyer/dev/datasets/corpus_of_cthulhu/chunks-bge_encode.jsonl
# If you are having trouble with the model import run this with your venv: python -m spacy download <name of your model>
# See: https://spacy.io/usage/models
SPACY_MODEL=en_core_web_sm

# Encoding and chunking scripts as well as our ElasticSearch service will need these values.
# Embedding dimensiona are found in the model documentation and should match what the model calls for.
EMBEDDING_DIMS=768
EMBEDDING_MODEL=BAAI/bge-base-en-v1.5

#ElasticSearch used by agent and when ingesting data. Do not include this in production, rather inject from your secret store.
# Also, you really will want to modify the client config to use proper security. This is just a local personal project for me.
ES_HOST=https://localhost:30920
ES_USER=elastic
ES_PASSWORD=changeme
ES_INDEX=lovecraft_chunks

# If false, we don't bother with the vLLM template or starting it in K8s from our Makefile.
SELF_HOST_LLM=false
# The following variables only matter if you are on a Linux machine with GPU support and plan to not use ollama or 3rd parties
LLM_MODEL_DIR=/opt/eldritch_oracle/models/llama3
LLM_MODEL_FILE=Llama-3.2-3B-Instruct-Q4_K_M.gguf
LLM_MODEL_PATH=${LLM_MODEL_DIR}/${LLM_MODEL_FILE}
LLM_MODEL_URL=https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf

# These are used by the API to call the language server with an OpenAI compatible API
INFERENCE_API_URL=http://127.0.0.1:11434
#INFERENCE_API_URL=http://localhost:11434 # Ollama default. Set to http://localhost:30800 if using vLLM setup
# dummy value for Ollama, real key for OpenAI or other OpenAI API compatible service. In production you should get rid of this and inject from your secret store.
INFERENCE_API_KEY=ollama
INFERENCE_MODEL_NAME=llama3.2:latest

# Chat options
# We return this many result chunks to include in the prompt. I set it really low because of the limited
# context window on the model I'm running locally. If it's too long, you lose the beginning of the system
# prompt and thus, the instructions to the model.
TOP_K_ES_RESULTS=2

# Logging configuration
LOG_LEVEL=INFO # DEBUG, INFO, WARNING, ERROR, CRITICAL

