#These are used when processing the data into chunks by teh prep_docs script
CORPUS_RAW_FILE_DIR=/Users/jennifersawyer/dev/datasets/corpus_of_cthulhu/txts
CORPUS_JSONL_FILE=/Users/jennifersawyer/dev/datasets/corpus_of_cthulhu/chunks-bge_encode.jsonl

#Encoding and chunking scripts as well as our ElasticSearch service will need these values.
EMBEDDING_DIMS=768
EMBEDDING_MODEL=BAAI/bge-base-en-v1.5

#ElasticSearch used by agent and when ingesting data. Do not include this in production, rather inject from your secret store.
# Also, you really will want to modify the client config to use proper security.
ES_HOST=https://localhost:30920
ES_USER=elastic
ES_PASSWORD=changeme
ES_INDEX=lovecraft_chunks


SELF_HOST_LLM=false
# The following variables only matter if you are on a Linux machine with GPU support and plan to not use ollama or 3rd parties
LLM_MODEL_DIR=/opt/eldritch_oracle/models/llama3
LLM_MODEL_FILE=Llama-3.2-3B-Instruct-Q4_K_M.gguf
LLM_MODEL_PATH=${LLM_MODEL_DIR}/${LLM_MODEL_FILE}
LLM_MODEL_URL=https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf

# These are used by the API to call the language server
INFERENCE_API_URL=http://localhost:11434 # Ollama default. Set to http://localhost:30800 if using vLLM setup
INFERENCE_API_KEY=ollama  # dummy value for Ollama, real key for OpenAI or other OpenAI API compatible service. In production you should get rid of this and inject from your secret store.
INFERENCE_MODEL_NAME=llama3.2


# Logging configuration
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL

